{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Extract the 'article' column for analysis\n",
    "headlines =  merged_df['article'].fillna('')  # Fill NaN with empty strings\n",
    "\n",
    "# Step 2: Define Keywords for Each Category\n",
    "categories = {\n",
    "    'Breaking News': ['breaking', 'alert', 'urgent'],\n",
    "    'Politics': ['election', 'policy', 'congress', 'senate'],\n",
    "    'World News': ['international', 'global', 'UN', 'war', 'diplomacy'],\n",
    "    'Business/Finance': ['market', 'stocks', 'finance', 'economy'],\n",
    "    'Technology': ['tech', 'software', 'hardware', 'AI', 'cyber'],\n",
    "    'Science': ['research', 'study', 'scientific', 'discovery'],\n",
    "    'Health': ['health', 'medicine', 'wellness', 'disease'],\n",
    "    'Entertainment': ['celebrity', 'movie', 'TV', 'music'],\n",
    "    'Sports': ['match', 'game', 'tournament', 'player'],\n",
    "    'Environment': ['climate', 'ecology', 'environment'],\n",
    "    'Crime': ['crime', 'arrest', 'police', 'court','kill','kidnapped'],\n",
    "    'Education': ['school', 'university', 'education', 'study'],\n",
    "    'Weather': ['weather', 'storm', 'hurricane', 'forecast'],\n",
    "    'Other': []  # Default category for unmatched articles\n",
    "}\n",
    "\n",
    "# Step 3: Function to Tag Articles Based on Keywords\n",
    "def assign_category(article):\n",
    "    for category, keywords in categories.items():\n",
    "        if any(keyword in article.lower() for keyword in keywords):\n",
    "            return category\n",
    "    return 'Other'  # Default category\n",
    "\n",
    "# Apply the function to assign categories\n",
    "merged_df['predicted_category'] =  merged_df['article'].apply(assign_category)\n",
    "\n",
    "# Step 4: TF-IDF Vectorization and Apply LSA\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "lsa = TruncatedSVD(n_components=100)\n",
    "\n",
    "# Create a pipeline for SVM classification after LSA\n",
    "classifier = SVC(kernel='linear')\n",
    "model = make_pipeline(tfidf, lsa, classifier)\n",
    "\n",
    "# Prepare data for training and testing\n",
    "labels = merged_df['predicted_category'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(headlines, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the classifier\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Extract Title and Article Texts and Use a Smaller Sample\n",
    "# Use only a sample of 10,000 rows to avoid memory issues\n",
    "sampled_df = merged_df.sample(n=10000, random_state=42)\n",
    "titles = sampled_df['title'].fillna('')\n",
    "articles = sampled_df['article'].fillna('')\n",
    "\n",
    "# Step 2: TF-IDF Vectorization with Reduced Number of Features\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)  # Reduce to 500 features\n",
    "\n",
    "# Combine the titles and articles for joint fitting\n",
    "combined_texts = titles.tolist() + articles.tolist()\n",
    "\n",
    "# Fit and transform the combined texts\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(combined_texts)\n",
    "\n",
    "# Separate the transformed vectors back into titles and articles\n",
    "tfidf_titles = tfidf_matrix[:len(titles)]\n",
    "tfidf_articles = tfidf_matrix[len(titles):]\n",
    "\n",
    "# Step 3: Compute Cosine Similarity\n",
    "# Calculate cosine similarity between each title and its corresponding article\n",
    "similarity_scores = cosine_similarity(tfidf_titles, tfidf_articles)\n",
    "\n",
    "# Extract the diagonal similarity scores\n",
    "similarity_scores_diagonal = [similarity_scores[i, i] for i in range(len(titles))]\n",
    "\n",
    "# Step 4: Add the Similarity Scores to the DataFrame\n",
    "sampled_df['similarity_score'] = similarity_scores_diagonal\n",
    "\n",
    "# Print the DataFrame with the new similarity scores\n",
    "print(sampled_df[['title', 'article', 'similarity_score']].head())\n",
    "\n",
    "# Step 5: Visualize the Similarity Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sampled_df['similarity_score'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Similarity Scores between Titles and Articles')\n",
    "plt.xlabel('Similarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Analyze the Average and Median Similarity Scores\n",
    "average_similarity = sampled_df['similarity_score'].mean()\n",
    "median_similarity = sampled_df['similarity_score'].median()\n",
    "\n",
    "print(f\"Average Similarity Score: {average_similarity:.2f}\")\n",
    "print(f\"Median Similarity Score: {median_similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorise the title/content into a known set of topic categories\n",
    "##### The main goal is to take these titles and sort them into different groups based on what they are about. These groups are called topic categories.we use it by using Topic modeling which is a technique used to discover the underlying themes or topics in a collection of documents. It involves analyzing the text data to identify patterns and group similar documents based on their content. The process typically includes several steps, with data preparation and preprocessing being crucial for effective topic modeling. We are using a method called Latent Dirichlet Allocation (LDA) to help us with this sorting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim import matutils  \n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1\n",
    "###  Data Preparation and Preprocessing for Topic Modeling\n",
    "##### here we do do data Normalization to ensures uniformity, which helps in accurately identifying words regardless of their case.and we remove noises so that we focus on the actual words that contribute to the meaning of the text.we also so Stopword  Stopwords.at last we Merg the 'title' and 'article' columns provides a more comprehensive context for each document, and prerocessed it so that it can lead us to better topic identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase(Normalization)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters using regex(Noise removal)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove stopwords (common words like \"and,\" \"the,\" \"is\") removal,which do not carry significant meaning and can skew the results)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Combine 'title' and 'article' into a single column\n",
    "merged_df['combined_text'] = merged_df['title'] + \" \" + merged_df['article']\n",
    "\n",
    "# Convert to a list for processing\n",
    "merged_df['processed_text'] = merged_df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "print(\"processed_text:\")\n",
    "print(merged_df['processed_text'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-2\n",
    "### we use tfidf_vectorizer to helps in reducing dimensionality and focusing on the most significant terms by limiting the nuique words(features number) and we use fit_transform to it so that we can fit or learn the vocabulary and IDF values from the processed data and transform text data to sparse matrix ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: TF-IDF Vectorization ###\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust max_features based on your dataset size\n",
    "\n",
    "# Transform the processed text data into TF-IDF vectors\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(merged_df['processed_text'])\n",
    "\n",
    "print(\"\\nTF-IDF Matrix Shape:\")\n",
    "print(tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: LDA Topic Modeling ###\n",
    "\n",
    "# Convert the TF-IDF matrix to a format compatible with Gensim\n",
    "corpus = matutils.Sparse2Corpus(tfidf_matrix, documents_columns=False)\n",
    "\n",
    "# Create a Gensim dictionary from the processed text\n",
    "id2word = corpora.Dictionary([text.split() for text in merged_df['processed_text']])\n",
    "\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=3, random_state=42, passes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Interpret and Categorize Results ###\n",
    "\n",
    "# Print the topics discovered by the LDA model\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"\\nTopic #{idx}:\")\n",
    "    print(topic)\n",
    "\n",
    "# Categorize documents by their dominant topic\n",
    "document_topics = [max(lda_model.get_document_topics(doc), key=lambda x: x[1])[0] for doc in corpus]\n",
    "merged_df['topic_category'] = document_topics\n",
    "\n",
    "print(\"\\nCategorized Documents with Topics:\")\n",
    "print(merged_df[['title', 'topic_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Interpret and Categorize Results ###\n",
    "\n",
    "# Print the topics discovered by the LDA model\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"\\nTopic #{idx}:\")\n",
    "    print(topic)\n",
    "\n",
    "# Categorize documents by their dominant topic\n",
    "document_topics = [max(lda_model.get_document_topics(doc), key=lambda x: x[1])[0] for doc in corpus]\n",
    "merged_df['topic_category'] = document_topics\n",
    "\n",
    "print(\"\\nCategorized Documents with Topics:\")\n",
    "print(merged_df[['title', 'topic_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Step 4: Visualize Topics with pyLDAvis ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare data for pyLDAvis\n",
    "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "\n",
    "# Display the interactive visualization within the notebook\n",
    "pyLDAvis.display(lda_display)\n",
    "\n",
    "### Step 5: Plot Topic Distribution Across Documents ###\n",
    "\n",
    "# Get topic distribution for each document\n",
    "document_topics = [lda_model.get_document_topics(doc) for doc in corpus]\n",
    "\n",
    "# Prepare data for plotting\n",
    "topic_distribution = pd.DataFrame([[topic[1] for topic in doc] for doc in document_topics])\n",
    "topic_distribution.columns = [f'Topic {i+1}' for i in range(topic_distribution.shape[1])]\n",
    "\n",
    "# Plot the topic distribution\n",
    "topic_distribution.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Topic Proportion')\n",
    "plt.title('Topic Distribution Across Documents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyzing the diversity of topics reported by different websites. \n",
    "## step-1\n",
    "so to answer the question Which websites reported the most diverse topics,we have to Assign Topics to Articles Using the topic modeling results of the above to determine the most dominant topic for each article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "from gensim import corpora\n",
    "\n",
    "def extract_main_domain(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        # Remove the 'www.' prefix if it exists\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "        return domain\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Apply the function to extract the main domain from the URL column\n",
    "merged_df['Domain'] = merged_df['url'].apply(extract_main_domain)\n",
    "\n",
    "# Assuming `X` is a list of lists, where each sublist is a document represented by its tokens\n",
    "# Example: X = [['word1', 'word2'], ['word3', 'word4'], ...]\n",
    "\n",
    "# Create a dictionary from the data (mapping of word IDs to words)\n",
    "id2word = corpora.Dictionary(X)\n",
    "\n",
    "# Convert the corpus to a bag-of-words format\n",
    "corpus_bow = [id2word.doc2bow(doc) for doc in X]\n",
    "\n",
    "# Get the most dominant topic for each document\n",
    "def get_dominant_topic(topic_distribution):\n",
    "    return np.argmax(topic_distribution)\n",
    "\n",
    "# Calculate topic distribution for each document\n",
    "document_topics = [lda_model.get_document_topics(doc) for doc in corpus_bow]\n",
    "topic_distribution = np.array([[topic[1] for topic in doc] for doc in document_topics])\n",
    "dominant_topics = [get_dominant_topic(doc_topics) for doc_topics in topic_distribution]\n",
    "\n",
    "# Add the dominant topic to the DataFrame\n",
    "merged_df['dominant_topic'] = dominant_topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step-2 \n",
    "We aggregate Data by Domain which means Group articles by their domains and calculate how many unique topics are reported by each domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the function to calculate topic diversity\n",
    "def calculate_topic_diversity(topic_counts):\n",
    "    if isinstance(topic_counts, dict) and len(topic_counts) > 0:\n",
    "        return len([count for count in topic_counts.values() if count > 0])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Group by 'Domain' and count unique topics in 'dominant_topic'\n",
    "domain_topic_summary = merged_df.groupby('domain')['dominant_topic'].apply(\n",
    "    lambda x: x.value_counts().to_dict()  # Convert to dictionary of topic counts\n",
    ").reset_index()\n",
    "\n",
    "# Remove the extra 'level_1' column\n",
    "domain_topic_summary = domain_topic_summary.drop(columns=['level_1'])\n",
    "\n",
    "# Apply the calculate_topic_diversity function\n",
    "domain_topic_summary['Topic_Diversity'] = domain_topic_summary['dominant_topic'].apply(calculate_topic_diversity)\n",
    "\n",
    "# Drop the 'dominant_topic' column since it's no longer needed\n",
    "domain_topic_summary = domain_topic_summary.drop(columns=['dominant_topic'])\n",
    "\n",
    "# Sort by 'Topic_Diversity'\n",
    "domain_topic_summary = domain_topic_summary.sort_values(by='Topic_Diversity', ascending=False)\n",
    "\n",
    "# Print the final output\n",
    "print(domain_topic_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora import MmCorpus\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Sample merged_df DataFrame setup (Replace this with actual data loading)\n",
    "# merged_df = pd.read_csv('path_to_your_data.csv')\n",
    "\n",
    "# Define the function to extract the main domain from a URL\n",
    "def extract_main_domain(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "        return domain\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Apply the domain extraction function to the URL column\n",
    "merged_df['domain'] = merged_df['url'].apply(extract_main_domain)\n",
    "\n",
    "# Combine 'title' and 'article' into one text column for topic modeling\n",
    "merged_df['text'] = merged_df['title'] + ' ' + merged_df['article']\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Apply tokenization\n",
    "documents = merged_df['text'].map(tokenize).tolist()\n",
    "\n",
    "# Create a Gensim Dictionary\n",
    "id2word = Dictionary(documents)\n",
    "\n",
    "# Create a Gensim Corpus\n",
    "corpus = [id2word.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Step 2: Train LDA Model\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=id2word, passes=15)\n",
    "\n",
    "# Step 3: Assign Topics to Documents\n",
    "def get_dominant_topic(ldamodel, corpus):\n",
    "    topics = [ldamodel.get_document_topics(doc) for doc in corpus]\n",
    "    dominant_topics = [max(doc, key=lambda x: x[1])[0] if doc else None for doc in topics]\n",
    "    return dominant_topics\n",
    "\n",
    "merged_df['dominant_topic'] = get_dominant_topic(lda_model, corpus)\n",
    "\n",
    "# Step 4: Calculate Topic Diversity for Each Domain\n",
    "def calculate_topic_diversity(topic_counts):\n",
    "    if isinstance(topic_counts, dict):\n",
    "        return len(topic_counts)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Group by domain and calculate topic counts\n",
    "domain_topic_summary = merged_df.groupby('domain')['dominant_topic'].apply(\n",
    "    lambda x: x.value_counts().to_dict()  # Convert to dictionary of topic counts\n",
    ")\n",
    "\n",
    "# Apply calculate_topic_diversity to get topic diversity counts\n",
    "domain_topic_summary = domain_topic_summary.apply(calculate_topic_diversity).reset_index()\n",
    "\n",
    "# Check the columns before renaming\n",
    "print(domain_topic_summary.columns)\n",
    "\n",
    "# Rename columns based on the actual number of columns\n",
    "if len(domain_topic_summary.columns) == 2:\n",
    "    domain_topic_summary.columns = ['Domain', 'Topic_Diversity']\n",
    "else:\n",
    "    print(\"Unexpected number of columns:\", len(domain_topic_summary.columns))\n",
    "    # Adjust renaming according to the actual column names\n",
    "\n",
    "# Sort by topic diversity\n",
    "domain_topic_summary = domain_topic_summary.sort_values(by='Topic_Diversity', ascending=False)\n",
    "\n",
    "print(domain_topic_summary)\n",
    "\n",
    "# Step 5: Visualize Topic Distribution Across Documents\n",
    "# Prepare data for plotting\n",
    "topic_distribution = pd.DataFrame(\n",
    "    [dict(ldamodel.get_document_topics(doc)) for doc in corpus]\n",
    ").fillna(0)  # Replace NaN with 0\n",
    "\n",
    "# Plot the topic distribution\n",
    "topic_distribution.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Topic Proportion')\n",
    "plt.title('Topic Distribution Across Documents')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-3\n",
    "Analyze Diversity and Determine which websites cover the most diverse range of topics and visualize this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \tModel the events that the news articles are written about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_main_domain(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        # Remove the 'www.' prefix if it exists\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "        return domain\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Apply the function to extract the main domain from the URL column\n",
    "da_rating['Domain'] =da_rating['url'].apply(extract_main_domain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "even if i classified to batch with size 32 because i am usinig 8GBram , it is hard to do it. so i just put the code here and in future i will make sure that i have a laptop with enough capability of doing it,so because of that i cand do any plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AlbertTokenizer\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "\n",
    "# Assuming da_rating is your DataFrame and has already been cleaned\n",
    "# Randomly sample 100 entries\n",
    "sampled_data = da_rating.sample(n=100, random_state=42)  # Ensure reproducibility with random_state\n",
    "\n",
    "# Extract texts and labels from the sampled data\n",
    "texts = sampled_data['content'].tolist()\n",
    "labels = sampled_data['category'].tolist()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "# Tokenize the sampled texts\n",
    "encoded_dict = tokenizer.batch_encode_plus(\n",
    "    texts,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    padding='max_length',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Convert lists of tensors to PyTorch tensors\n",
    "input_ids = encoded_dict['input_ids']\n",
    "attention_masks = encoded_dict['attention_mask']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "encoded_labels = torch.tensor(encoded_labels)\n",
    "\n",
    "# Verify lengths and shapes\n",
    "print(f\"Number of samples: {len(texts)}\")\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention Masks shape: {attention_masks.shape}\")\n",
    "print(f\"Encoded Labels shape: {encoded_labels.shape}\")\n",
    "\n",
    "# Now you can use input_ids, attention_masks, and encoded_labels for model training or testing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids, encoded_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "train_masks, val_masks = train_test_split(\n",
    "    attention_masks, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to TensorDataset\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training set\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=16  # You can adjust the batch size\n",
    ")\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=16  # You can adjust the batch size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertForSequenceClassification\n",
    "\n",
    "# Load the ALBERT model for sequence classification\n",
    "model = AlbertForSequenceClassification.from_pretrained(\n",
    "    'albert-base-v2',\n",
    "    num_labels=len(label_encoder.classes_),  # Number of unique labels\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,  # Default value in transformers\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Set up the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch_inputs = batch[0].to(device)\n",
    "        batch_masks = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=batch_inputs,\n",
    "            attention_mask=batch_masks,\n",
    "            labels=batch_labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0 to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f'Average Training Loss: {avg_train_loss:.2f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch_inputs = batch[0].to(device)\n",
    "            batch_masks = batch[1].to(device)\n",
    "            batch_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=batch_inputs,\n",
    "                attention_mask=batch_masks,\n",
    "                labels=batch_labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "            correct_predictions += (preds == batch_labels).cpu().numpy().sum()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_predictions / len(val_labels)\n",
    "    print(f'Validation Loss: {avg_val_loss:.2f}')\n",
    "    print(f'Validation Accuracy: {val_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Sample 500 news articles (assuming da_rating is your DataFrame)\n",
    "sampled_articles = da_rating.sample(n=500, random_state=42)\n",
    "\n",
    "# Extract the main domain from URLs\n",
    "def extract_main_domain(url):\n",
    "    try:\n",
    "        domain = urlparse(url).netloc\n",
    "        return domain.split('.')[0]\n",
    "    except Exception as e:\n",
    "        return \"Unknown\"\n",
    "\n",
    "sampled_articles['domain'] = sampled_articles['url'].apply(extract_main_domain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts for prediction\n",
    "texts_to_predict = sampled_articles['content'].tolist()\n",
    "\n",
    "# Tokenize the texts\n",
    "encoded_dict = tokenizer.batch_encode_plus(\n",
    "    texts_to_predict,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    padding='max_length',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "input_ids = encoded_dict['input_ids'].to(device)\n",
    "attention_masks = encoded_dict['attention_mask'].to(device)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "# Decode labels\n",
    "predicted_labels = label_encoder.inverse_transform(predictions.cpu().numpy())\n",
    "sampled_articles['predicted_category'] = predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggregate data to count occurrences of each event by domain\n",
    "event_counts = sampled_articles.groupby(['domain', 'predicted_category']).size().reset_index(name='counts')\n",
    "\n",
    "# Pivot the data for visualization\n",
    "pivot_table = event_counts.pivot(index='domain', columns='predicted_category', values='counts').fillna(0)\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(pivot_table, cmap=\"YlGnBu\", annot=True, fmt=\"g\")\n",
    "plt.title('Event Counts by Media Domain')\n",
    "plt.xlabel('Event Category')\n",
    "plt.ylabel('Media Domain')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●\tHow many events are covered in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Vectorize the content\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(sampled_articles['content'])\n",
    "\n",
    "# Perform clustering\n",
    "num_clusters = 5  # Example number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "sampled_articles['cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'published_at' to datetime\n",
    "sampled_articles['published_at'] = pd.to_datetime(sampled_articles['published_at'])\n",
    "\n",
    "# Find the earliest report for each event\n",
    "earliest_reports = sampled_articles.groupby('cluster')['published_at'].min()\n",
    "\n",
    "# Merge with the original DataFrame to get the news sites\n",
    "earliest_reports_df = pd.merge(sampled_articles, earliest_reports, on=['cluster', 'published_at'])\n",
    "\n",
    "# Find the earliest reporting news sites for each event\n",
    "earliest_reports_sites = earliest_reports_df[['cluster', 'domain', 'published_at']].drop_duplicates()\n",
    "print(earliest_reports_sites)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each event\n",
    "event_reporting_counts = sampled_articles['cluster'].value_counts()\n",
    "print(\"Events with the highest reporting:\")\n",
    "print(event_reporting_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table to create a matrix of event counts by domain\n",
    "pivot_table = sampled_articles.pivot_table(index='domain', columns='cluster', values='article_id', aggfunc='count', fill_value=0)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = pivot_table.corr()\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=True)\n",
    "plt.title('Correlation Between News Sites Reporting Events')\n",
    "plt.xlabel('Event Clusters')\n",
    "plt.ylabel('News Sites')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
